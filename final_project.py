# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14YmtOSqbcI91fJwSMO6PTq5thZsoHUX4

## DATA COLLECTION ‚Äì IMDb Dataset
"""

# data_loading.py

import torch
import random
import numpy as np
from datasets import load_dataset

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(42)

# Load IMDb dataset
dataset = load_dataset("imdb")

train_data = dataset["train"]
test_data = dataset["test"]

print(train_data[0])

"""## DATA PREPROCESSING FOR CUSTOM LSTM"""

# preprocessing_lstm.py

import re
import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter

MAX_VOCAB_SIZE = 20000
MAX_LEN = 200

def clean_text(text):
    text = text.lower()
    text = re.sub(r"<.*?>", "", text)
    text = re.sub(r"[^a-zA-Z]", " ", text)
    return text

def tokenize(text):
    return text.split()

def build_vocab(dataset):
    counter = Counter()
    for item in dataset:
        text = clean_text(item["text"])
        tokens = tokenize(text)
        counter.update(tokens)
    vocab = {"<PAD>": 0, "<UNK>": 1}
    for word, freq in counter.most_common(MAX_VOCAB_SIZE - 2):
        vocab[word] = len(vocab)
    return vocab

vocab = build_vocab(train_data)

def encode(text):
    tokens = tokenize(clean_text(text))
    ids = [vocab.get(token, vocab["<UNK>"]) for token in tokens]
    if len(ids) < MAX_LEN:
        ids += [vocab["<PAD>"]] * (MAX_LEN - len(ids))
    else:
        ids = ids[:MAX_LEN]
    return ids

class IMDBDataset(Dataset):
    def __init__(self, dataset):
        self.texts = [encode(item["text"]) for item in dataset]
        self.labels = [item["label"] for item in dataset]

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])

train_dataset = IMDBDataset(train_data)
test_dataset = IMDBDataset(test_data)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

"""## CUSTOM LSTM MODEL"""

# custom_lstm_model.py

import torch.nn as nn

class CustomLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, output_dim=1):
        super(CustomLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        embedded = self.embedding(x)
        _, (hidden, _) = self.lstm(embedded)
        output = self.fc(hidden[-1])
        return self.sigmoid(output)

model = CustomLSTM(len(vocab))

"""## TRAINING LOOP"""

# train_lstm.py

import torch
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

EPOCHS = 5

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for texts, labels in train_loader:
        texts, labels = texts.to(device), labels.float().to(device)

        optimizer.zero_grad()
        outputs = model(texts).squeeze()
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}")

"""## EVALUATION"""

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for texts, labels in test_loader:
        texts = texts.to(device)
        outputs = model(texts).squeeze()
        preds = torch.round(outputs).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.numpy())

acc = accuracy_score(all_labels, all_preds)
precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')

print("Accuracy:", acc)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""## 4  AWD-LSTM (ULMFiT) USING FASTAI"""

# ulmfit_model.py

from fastai.text.all import *
from fastprogress.fastprogress import force_console_behavior

# ‚úÖ Disable notebook progress bar (fix error)
force_console_behavior()

# Load dataset
path = untar_data(URLs.IMDB)

# Create DataLoaders
dls = TextDataLoaders.from_folder(path, valid='test')

# Create learner
learn = text_classifier_learner(
    dls,
    AWD_LSTM,
    drop_mult=0.5,
    metrics=accuracy
)

# Fine tune model
learn.remove_cb(ProgressCallback)
learn.fine_tune(4)


# Save model
learn.save("ulmfit_model")

print("Training Completed Successfully ‚úÖ")

"""## 5 BERT MODEL (TRANSFORMER)"""

# bert_model.py

from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
import numpy as np

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=256
    )

encoded_dataset = dataset.map(tokenize_function, batched=True)
encoded_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

training_args = TrainingArguments(
    output_dir="./results",
    # evaluation_strategy="epoch", # This argument is not recognized by the current transformers version.
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["test"],
)

trainer.train()

"""## COMMON METRIC FUNCTION"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
import matplotlib.pyplot as plt

def evaluate_model(true_labels, predictions):
    acc = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions)
    recall = recall_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions)

    return acc, precision, recall, f1

"""## CUSTOM LSTM EVALUATION"""

lstm_acc, lstm_precision, lstm_recall, lstm_f1 = evaluate_model(all_labels, all_preds)

print("Custom LSTM Results")
print("Accuracy:", lstm_acc)
print("Precision:", lstm_precision)
print("Recall:", lstm_recall)
print("F1 Score:", lstm_f1)

"""## ULMFiT (AWD-LSTM) EVALUATION"""

ulmfit_preds, ulmfit_targets = learn.get_preds()

ulmfit_preds_labels = ulmfit_preds.argmax(dim=1).numpy()
ulmfit_true = ulmfit_targets.numpy()

ulmfit_acc, ulmfit_precision, ulmfit_recall, ulmfit_f1 = evaluate_model(
    ulmfit_true,
    ulmfit_preds_labels
)

print("ULMFiT Results")
print("Accuracy:", ulmfit_acc)
print("Precision:", ulmfit_precision)
print("Recall:", ulmfit_recall)
print("F1 Score:", ulmfit_f1)

"""## BERT EVALUATION"""

bert_results = trainer.predict(encoded_dataset["test"])

bert_preds = np.argmax(bert_results.predictions, axis=1)
bert_true = bert_results.label_ids

bert_acc, bert_precision, bert_recall, bert_f1 = evaluate_model(
    bert_true,
    bert_preds
)

print("BERT Results")
print("Accuracy:", bert_acc)
print("Precision:", bert_precision)
print("Recall:", bert_recall)
print("F1 Score:", bert_f1)

"""## FINAL COMPARISON TABLE"""

comparison_df = pd.DataFrame({
    "Model": ["Custom LSTM", "ULMFiT (AWD-LSTM)", "BERT"],
    "Accuracy": [lstm_acc, ulmfit_acc, bert_acc],
    "Precision": [lstm_precision, ulmfit_precision, bert_precision],
    "Recall": [lstm_recall, ulmfit_recall, bert_recall],
    "F1 Score": [lstm_f1, ulmfit_f1, bert_f1]
})

print("\nFinal Model Comparison:")
print(comparison_df)

"""## PERFORMANCE VISUALIZATION"""

comparison_df.set_index("Model")[["Accuracy", "F1 Score"]].plot(kind="bar")
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.ylim(0, 1)
plt.xticks(rotation=0)
plt.show()

# ===============================
# FINAL MODEL COMPARISON
# ===============================

comparison = {
    "Custom LSTM": lstm_acc,
    "ULMFiT": ulmfit_acc,
    "BERT": bert_acc
}

best_model = max(comparison, key=comparison.get)

print("\n==============================")
print("FINAL MODEL PERFORMANCE")
print("==============================")

for model, score in comparison.items():
    print(f"{model} Accuracy: {score:.4f}")

print("\nüèÜ Best Performing Model:", best_model)

if best_model == "BERT":
    print("Conclusion: BERT outperforms Custom LSTM and ULMFiT in Sentiment Analysis.")

