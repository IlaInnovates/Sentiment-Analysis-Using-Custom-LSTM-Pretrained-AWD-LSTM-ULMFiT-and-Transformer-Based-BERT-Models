# -*- coding: utf-8 -*-
"""final_proj6.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T40m3Znsl3OD8MnM31PR2rsKN4MfNYP3

# PART A — CUSTOM LSTM (FROM SCRATCH, PYTORCH)

## STEP 1: Import Required Libraries
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from datasets import load_dataset
import re
from collections import Counter

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

"""## STEP 2: Load IMDb Dataset"""

dataset = load_dataset("imdb")
train_data = dataset["train"]
test_data = dataset["test"]

"""## STEP 3: Text Preprocessing (Tokenization)"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    return text.split()

"""## STEP 4: Build Vocabulary"""

counter = Counter()

for text in train_data["text"]:
    counter.update(clean_text(text))

vocab = {"<PAD>": 0, "<UNK>": 1}

for word, freq in counter.items():
    if freq >= 5:
        vocab[word] = len(vocab)

vocab_size = len(vocab)
MAX_LEN = 200

"""## STEP 5: Encode & Pad Sequences"""

def encode(text):
    tokens = clean_text(text)
    ids = [vocab.get(w, vocab["<UNK>"]) for w in tokens]
    ids = ids[:MAX_LEN]
    ids += [0] * (MAX_LEN - len(ids))
    return ids

"""## STEP 6: Custom Dataset Class"""

class IMDBDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        x = torch.tensor(encode(self.texts[idx]), dtype=torch.long)
        y = torch.tensor(self.labels[idx], dtype=torch.float)
        return x, y

"""## STEP 7: DataLoaders"""

train_ds = IMDBDataset(train_data["text"], train_data["label"])
test_ds  = IMDBDataset(test_data["text"], test_data["label"])

train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader  = DataLoader(test_ds, batch_size=32)

"""## STEP 8: Build Custom LSTM Model"""

class CustomLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = self.embedding(x)
        _, (hidden, _) = self.lstm(x)
        return self.fc(hidden[-1]).squeeze()

"""## STEP 9: Train Custom LSTM"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = CustomLSTM(vocab_size).to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

EPOCHS = 5

for epoch in range(EPOCHS):
    model.train()
    total_loss = 0

    for x, y in train_loader:
        x, y = x.to(device), y.to(device)

        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{EPOCHS} Loss: {total_loss/len(train_loader):.4f}")

"""## STEP 10: Evaluate Custom LSTM"""

model.eval()
preds, labels = [], []

with torch.no_grad():
    for x, y in test_loader:
        x = x.to(device)
        output = torch.sigmoid(model(x)) > 0.5

        preds.extend(output.cpu().tolist())
        labels.extend(y.tolist())

custom_lstm_results = {
    "accuracy": accuracy_score(labels, preds),
    "precision": precision_score(labels, preds),
    "recall": recall_score(labels, preds),
    "f1": f1_score(labels, preds)
}

print(custom_lstm_results)

model.eval()
val_loss = 0

with torch.no_grad():
    for x, y in test_loader:
        x, y = x.to(device), y.to(device)
        output = model(x)
        loss = criterion(output, y)
        val_loss += loss.item()

val_loss /= len(test_loader)

print(
    f"Epoch {epoch+1}/{EPOCHS} | "
    f"Train Loss: {total_loss/len(train_loader):.4f} | "
    f"Val Loss: {val_loss:.4f}"
)

"""# PART B — PRETRAINED AWD-LSTM (ULMFIT)

## STEP 11: Install & Import fastai
"""

!pip install 'numpy<2'
!pip install fastai==2.7.14
from fastai.text.all import *

"""## STEP 12: Load IMDb Dataset Automatically"""

from fastai.data.all import *
path = untar_data(URLs.IMDB)

"""## STEP 13: Create DataLoaders"""

from fastai.text.data import TextDataLoaders
dls = TextDataLoaders.from_folder(
    path,
    train='train',
    valid='test',
    bs=64
)

"""## STEP 14: Load Pretrained AWD-LSTM"""

from fastai.text.all import *

learn = text_classifier_learner(
    dls,
    AWD_LSTM,
    metrics=accuracy
)

from fastai.metrics import accuracy, Precision, Recall, F1Score

learn = text_classifier_learner(
    dls,
    AWD_LSTM,
    metrics=[
        accuracy,
        Precision(average='binary'),
        Recall(average='binary'),
        F1Score(average='binary')

    ]
)

"""## STEP 15: Fine-Tune (ULMFiT)"""

learn.fine_tune(1)

"""## STEP 16: Evaluate AWD-LSTM"""

awd_lstm_accuracy = learn.validate()[1]
print("AWD-LSTM Accuracy:", awd_lstm_accuracy)

"""## STEP 17: Evaluate AWD-LSTM (All Metrics)"""

awd_results = learn.validate()
print("AWD-LSTM Metrics:")
print(f"Loss      : {awd_results[0]:.4f}")
print(f"Accuracy  : {awd_results[1]:.4f}")
print(f"Precision : {awd_results[2]:.4f}")
print(f"Recall    : {awd_results[3]:.4f}")
print(f"F1-Score  : {awd_results[4]:.4f}")

comparison = {
    "Model": ["Custom LSTM", "AWD-LSTM"],
    "Accuracy": [custom_lstm_results["accuracy"], awd_lstm_accuracy]
}

for i in range(2):
    print(f"{comparison['Model'][i]} → Accuracy: {comparison['Accuracy'][i]:.4f}")

