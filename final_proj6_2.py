# -*- coding: utf-8 -*-
"""final_proj6.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SwwZNFY6EgQWZa8T7Ai_pyLrLvFWQEvm

# PROJECT 2 â€” PRETRAINED TRANSFORMER (BERT)

## STEP 1: Install & Import Libraries
"""

!pip install transformers datasets accelerate

import torch
from datasets import load_dataset
from transformers import (
    BertTokenizerFast,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

"""## STEP 2: Load SAME IMDb Dataset"""

dataset = load_dataset("imdb")

"""## STEP 3: Tokenize Using BERT WordPiece Tokenizer"""

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

def tokenize_batch(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=128
    )

encoded_ds = dataset.map(tokenize_batch, batched=True)

"""## STEP 4: Prepare Dataset for Training"""

encoded_ds = encoded_ds.rename_column("label", "labels")
encoded_ds.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)

"""## STEP 5: Load Pretrained BERT Model"""

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

"""## STEP 6: Define Evaluation Metrics (Same Logic)"""

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=1)

    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average="binary"
    )
    acc = accuracy_score(labels, preds)

    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

"""## STEP 7: Training Configuration"""

training_args = TrainingArguments(
    output_dir="./bert_results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    save_total_limit=1,
    logging_steps=500,
    report_to="none"
)

"""## STEP 8: Trainer Setup"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_ds["train"],
    eval_dataset=encoded_ds["test"],
    compute_metrics=compute_metrics
)

"""## STEP 9: Fine-Tune BERT"""

trainer.train()

"""## STEP 10: Evaluate BERT"""

bert_results = trainer.evaluate()
print(bert_results)